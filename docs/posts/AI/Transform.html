<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.18" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.59" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://blog.vipfreevpn.top/posts/AI/Transform.html"><meta property="og:site_name" content="博客"><meta property="og:title" content="Transform"><meta property="og:description" content=""Attention is All You Need" 是2017年由Google Brain团队发表的一篇里程碑式论文，提出了Transformer模型架构。这个模型引入了注意力机制（Attention Mechanism），彻底改变了自然语言处理（NLP）领域的格局。以下是该论文的主要要点和影响： 主要贡献 纯注意力架构： Transformer完..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:published_time" content="2024-11-25T22:18:25.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Transform","image":[""],"datePublished":"2024-11-25T22:18:25.000Z","dateModified":null,"author":[{"@type":"Person","name":"HideOne","url":"https://mister-hope.com"}]}</script><title>Transform | 博客</title><meta name="description" content=""Attention is All You Need" 是2017年由Google Brain团队发表的一篇里程碑式论文，提出了Transformer模型架构。这个模型引入了注意力机制（Attention Mechanism），彻底改变了自然语言处理（NLP）领域的格局。以下是该论文的主要要点和影响： 主要贡献 纯注意力架构： Transformer完...">
    <link rel="preload" href="/assets/style-D4MzLsFf.css" as="style"><link rel="stylesheet" href="/assets/style-D4MzLsFf.css">
    <link rel="modulepreload" href="/assets/app-DROX2scZ.js"><link rel="modulepreload" href="/assets/Transform.html-CFljF92K.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-BqOjgHUp.js" as="script"><link rel="prefetch" href="/assets/数据库.html-B4st_-we.js" as="script"><link rel="prefetch" href="/assets/linux安装.html-Bqlc_V6H.js" as="script"><link rel="prefetch" href="/assets/shell.html-DHNLF14S.js" as="script"><link rel="prefetch" href="/assets/npm.html-Deq3VXEs.js" as="script"><link rel="prefetch" href="/assets/卷积神经网路.html-DXMU1qDE.js" as="script"><link rel="prefetch" href="/assets/神经网络结构.html-D88tQDhr.js" as="script"><link rel="prefetch" href="/assets/vscode插件.html-DOZ1a6Wi.js" as="script"><link rel="prefetch" href="/assets/mysql.html-CN7ST1oX.js" as="script"><link rel="prefetch" href="/assets/分支界限法.html-Beez9YBn.js" as="script"><link rel="prefetch" href="/assets/分治算法.html-Dwc02a8p.js" as="script"><link rel="prefetch" href="/assets/动态规划.html-zSOHIei_.js" as="script"><link rel="prefetch" href="/assets/回溯算法.html-XHZ3ZdMF.js" as="script"><link rel="prefetch" href="/assets/常用简单算法.html-DiWJXY5q.js" as="script"><link rel="prefetch" href="/assets/贪心算法.html-qotiOwZ6.js" as="script"><link rel="prefetch" href="/assets/设计模式.html-Bj-Q-nJZ.js" as="script"><link rel="prefetch" href="/assets/快捷键.html-CzWT8Mwp.js" as="script"><link rel="prefetch" href="/assets/php.html-DLznRFNH.js" as="script"><link rel="prefetch" href="/assets/享元模式.html-CEV62WPR.js" as="script"><link rel="prefetch" href="/assets/单例模式.html-Q-vWa94J.js" as="script"><link rel="prefetch" href="/assets/原型模式.html-uhiD8HSo.js" as="script"><link rel="prefetch" href="/assets/命令模式.html-BUGmDxer.js" as="script"><link rel="prefetch" href="/assets/沙盒模式.html-Ch6w2mrG.js" as="script"><link rel="prefetch" href="/assets/状态模式.html-DsdpMaxj.js" as="script"><link rel="prefetch" href="/assets/类型对象模式.html-D2Bvt6pr.js" as="script"><link rel="prefetch" href="/assets/装饰器模式.html-DP-LYcu5.js" as="script"><link rel="prefetch" href="/assets/观察者模式.html-u3tTcLdp.js" as="script"><link rel="prefetch" href="/assets/解释器模式.html-Cw-U_xUi.js" as="script"><link rel="prefetch" href="/assets/编辑器.html-BPYtPCIM.js" as="script"><link rel="prefetch" href="/assets/面试题.html-Dv5Kmqqf.js" as="script"><link rel="prefetch" href="/assets/光栅化.html-mAB7llns.js" as="script"><link rel="prefetch" href="/assets/变换.html-CYqd5P_q.js" as="script"><link rel="prefetch" href="/assets/线性代数.html-BR_e0Rxg.js" as="script"><link rel="prefetch" href="/assets/cocos 2d渲染.html-C60kfQFR.js" as="script"><link rel="prefetch" href="/assets/CocosShader.html-TFZ7J43E.js" as="script"><link rel="prefetch" href="/assets/GLSL.html-BCJC1HTk.js" as="script"><link rel="prefetch" href="/assets/LUT滤镜.html-CEuwuqcT.js" as="script"><link rel="prefetch" href="/assets/unityShader.html-CVLitUWg.js" as="script"><link rel="prefetch" href="/assets/光照.html-IzM3ojdv.js" as="script"><link rel="prefetch" href="/assets/屏幕后效.html-DH3f1A05.js" as="script"><link rel="prefetch" href="/assets/渲染管线.html-CraIkVMP.js" as="script"><link rel="prefetch" href="/assets/Camera.html-BkI9c78E.js" as="script"><link rel="prefetch" href="/assets/canvas.html-DxAbqNE3.js" as="script"><link rel="prefetch" href="/assets/HybridCLR热更.html-U41dHAUP.js" as="script"><link rel="prefetch" href="/assets/mvc.html-DZZTr-Yt.js" as="script"><link rel="prefetch" href="/assets/prefab.html-BivjTUYm.js" as="script"><link rel="prefetch" href="/assets/场景.html-B6MPqXgj.js" as="script"><link rel="prefetch" href="/assets/工具.html-BKCQ27SH.js" as="script"><link rel="prefetch" href="/assets/sql注入.html-vA7bA6hz.js" as="script"><link rel="prefetch" href="/assets/CNet.html-Bxla1nk3.js" as="script"><link rel="prefetch" href="/assets/donet框架.html-Cx4WVCEU.js" as="script"><link rel="prefetch" href="/assets/反射.html-BO2ULlWt.js" as="script"><link rel="prefetch" href="/assets/并发.html-DCPAAIob.js" as="script"><link rel="prefetch" href="/assets/方法扩展.html-DsK60w5H.js" as="script"><link rel="prefetch" href="/assets/特性.html-DU9FiTzi.js" as="script"><link rel="prefetch" href="/assets/html.html-ADi5pbLF.js" as="script"><link rel="prefetch" href="/assets/css.html-DvSanXZe.js" as="script"><link rel="prefetch" href="/assets/GLSL函数.html-C8cwbw3w.js" as="script"><link rel="prefetch" href="/assets/玻璃窗效果.html-Bq-xXrCf.js" as="script"><link rel="prefetch" href="/assets/404.html-iHlsNy8_.js" as="script"><link rel="prefetch" href="/assets/index.html-Cx6FQPf1.js" as="script"><link rel="prefetch" href="/assets/index.html-DRjiuUiQ.js" as="script"><link rel="prefetch" href="/assets/index.html-Ch8377CH.js" as="script"><link rel="prefetch" href="/assets/index.html-DF8qPAvG.js" as="script"><link rel="prefetch" href="/assets/index.html-BpkVGhvn.js" as="script"><link rel="prefetch" href="/assets/index.html-BehA77vw.js" as="script"><link rel="prefetch" href="/assets/index.html-B5RYf9S1.js" as="script"><link rel="prefetch" href="/assets/index.html-NvpLKquU.js" as="script"><link rel="prefetch" href="/assets/index.html-BKlG2cpW.js" as="script"><link rel="prefetch" href="/assets/index.html-DgWl1HYe.js" as="script"><link rel="prefetch" href="/assets/index.html-CpMK5f3t.js" as="script"><link rel="prefetch" href="/assets/index.html-DeMC_FN_.js" as="script"><link rel="prefetch" href="/assets/index.html-rQo2V0NZ.js" as="script"><link rel="prefetch" href="/assets/index.html-DPf68lnM.js" as="script"><link rel="prefetch" href="/assets/index.html-o6u7VSBC.js" as="script"><link rel="prefetch" href="/assets/index.html-DYLQ76i_.js" as="script"><link rel="prefetch" href="/assets/index.html-KEEreU80.js" as="script"><link rel="prefetch" href="/assets/index.html-DlWSDurX.js" as="script"><link rel="prefetch" href="/assets/index.html-D2WLLsTf.js" as="script"><link rel="prefetch" href="/assets/index.html-D8N5b4F6.js" as="script"><link rel="prefetch" href="/assets/index.html-hdSIM6DD.js" as="script"><link rel="prefetch" href="/assets/index.html-D3rYAO6i.js" as="script"><link rel="prefetch" href="/assets/index.html-BhuPHnYy.js" as="script"><link rel="prefetch" href="/assets/index.html-DyNN3NWF.js" as="script"><link rel="prefetch" href="/assets/index.html-Bg0yk05S.js" as="script"><link rel="prefetch" href="/assets/index.html-BU8foQ3l.js" as="script"><link rel="prefetch" href="/assets/index.html-CCz7iX0T.js" as="script"><link rel="prefetch" href="/assets/index.html-5QEAazgA.js" as="script"><link rel="prefetch" href="/assets/index.html-CKvYQOsX.js" as="script"><link rel="prefetch" href="/assets/index.html-BKbtH7bp.js" as="script"><link rel="prefetch" href="/assets/index.html-BoMbaGfZ.js" as="script"><link rel="prefetch" href="/assets/index.html-B44C55G_.js" as="script"><link rel="prefetch" href="/assets/index.html-Do0lFps5.js" as="script"><link rel="prefetch" href="/assets/index.html-E425_oHo.js" as="script"><link rel="prefetch" href="/assets/index.html-7p8QueuS.js" as="script"><link rel="prefetch" href="/assets/index.html-E425_oHo.js" as="script"><link rel="prefetch" href="/assets/index.html-CjD5EAKp.js" as="script"><link rel="prefetch" href="/assets/index.html-CLmJw1K3.js" as="script"><link rel="prefetch" href="/assets/index.html-Doo7HdVB.js" as="script"><link rel="prefetch" href="/assets/index.html-CjTXajrn.js" as="script"><link rel="prefetch" href="/assets/index.html-BrHHrC8e.js" as="script"><link rel="prefetch" href="/assets/index.html-BpEjbq-U.js" as="script"><link rel="prefetch" href="/assets/index.html-Dw5-KlB7.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-GXRgw7eJ.js" as="script"><link rel="prefetch" href="/assets/setupDevtools-7MC2TMWH-5myVbMXM.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="https://theme-hope-assets.vuejs.press/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">博客</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="博客主页"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->博客主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="博文"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>博文<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/数据库/数据库.html" aria-label="/posts/数据库/数据库.html"><!---->/posts/数据库/数据库.html<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/linux/linux%E5%AE%89%E8%A3%85.html" aria-label="linux安装"><!---->linux安装<!----></a></li></ul></button></div></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/git@github.com:HideOne/blog.github.io.git" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="博客主页"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->博客主页<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">文章</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">插件</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">建模</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">设计模式</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/%E6%95%B0%E6%8D%AE%E5%BA%93.html" aria-label="数据库"><!---->数据库<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">数据库</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">算法</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">网络安全</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">游戏</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">语言</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">AI</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/posts/AI/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF.html" aria-label="卷积神经网路(CNN)"><!---->卷积神经网路(CNN)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/AI/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.html" aria-label="神经网络结构"><!---->神经网络结构<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/posts/AI/Transform.html" aria-label="Transform"><!---->Transform<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Linux</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Nodejs</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Transform</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://mister-hope.com" target="_blank" rel="noopener noreferrer">HideOne</a></span><span property="author" content="HideOne"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2024年11月26日</span><meta property="datePublished" content="2024-11-25T22:18:25.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 12 分钟</span><meta property="timeRequired" content="PT12M"></span><!----><!----></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#主要贡献">主要贡献</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#影响">影响</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#关键点">关键点</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#总结">总结</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#主要特点">主要特点</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#架构">架构</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#应用">应用</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#影响-1">影响</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#总结-1">总结</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level1" href="#transformer">Transformer</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-的核心思想">Transformer 的核心思想</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-的架构">Transformer 的架构</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-的工作流程">Transformer 的工作流程</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#注意力机制的数学原理">注意力机制的数学原理</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-的特点和优势">Transformer 的特点和优势</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-的应用">Transformer 的应用</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-的局限性">Transformer 的局限性</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-的发展">Transformer 的发展</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#总结-2">总结</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level1" href="#注意力机制-attention">注意力机制 Attention</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#attention-注意力机制">Attention（注意力机制）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#attention-的直观理解">Attention 的直观理解</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#attention-的数学公式">Attention 的数学公式</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#attention-的分类">Attention 的分类</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#自注意力机制-self-attention">自注意力机制（Self-Attention）</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#注意力的优势">注意力的优势</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#注意力的局限性">注意力的局限性</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#注意力的应用场景">注意力的应用场景</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#总结-3">总结</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><p><strong>&quot;Attention is All You Need&quot;</strong> 是2017年由Google Brain团队发表的一篇里程碑式论文，提出了Transformer模型架构。这个模型引入了注意力机制（Attention Mechanism），彻底改变了自然语言处理（NLP）领域的格局。以下是该论文的主要要点和影响：</p><h3 id="主要贡献" tabindex="-1"><a class="header-anchor" href="#主要贡献"><span>主要贡献</span></a></h3><ol><li><p><strong>纯注意力架构</strong>：</p><ul><li>Transformer完全摒弃了传统的RNN/LSTM结构，仅使用自注意力机制（Self-Attention）来处理输入序列的依赖关系。</li><li>这种设计使得模型能够并行化处理输入，极大地加速了训练过程。</li></ul></li><li><p><strong>自注意力机制（Self-Attention）</strong>：</p><ul><li>通过计算输入序列中的每个词与其他所有词之间的关系，捕捉全局依赖。</li><li>公式为： [ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V ] 其中，( Q ), ( K ), ( V ) 分别是查询（Query）、键（Key）和值（Value），( d_k ) 是键的维度。</li></ul></li><li><p><strong>多头注意力（Multi-Head Attention）</strong>：</p><ul><li>通过并行计算多个注意力机制，模型可以从不同的表示空间中获取信息，增强了模型的表达能力。</li></ul></li><li><p><strong>位置编码（Positional Encoding）</strong>：</p><ul><li>由于Transformer没有时间序列的内置顺序信息，通过添加位置编码来保留序列中单词的位置信息。</li></ul></li><li><p><strong>Encoder-Decoder结构</strong>：</p><ul><li>Transformer由编码器和解码器组成，编码器处理输入序列，解码器生成输出序列。</li></ul></li></ol><h3 id="影响" tabindex="-1"><a class="header-anchor" href="#影响"><span>影响</span></a></h3><ul><li><strong>并行计算</strong>：Transformer的设计允许在训练和推理时进行并行计算，大大减少了训练时间。</li><li><strong>长距离依赖</strong>：自注意力机制能够捕捉长距离依赖，这在传统的RNN中是一个挑战。</li><li><strong>NLP领域的变革</strong>：Transformer成为了许多后续NLP模型的基础，如BERT、RoBERTa、XLNet等。</li><li><strong>广泛应用</strong>：不仅仅是NLP，Transformer也被应用于计算机视觉、语音处理等领域，证明了其通用性和强大的表现力。</li></ul><h3 id="关键点" tabindex="-1"><a class="header-anchor" href="#关键点"><span>关键点</span></a></h3><ul><li><strong>无递归结构</strong>：Transformer不使用递归或卷积来处理序列数据，而是通过自注意力机制捕获序列中的依赖关系。</li><li><strong>可扩展性</strong>：由于其并行处理能力，Transformer可以轻松处理更长序列的数据。</li><li><strong>注意力可视化</strong>：自注意力机制提供了直观的理解方式，研究人员可以可视化不同词语之间的注意力权重。</li></ul><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><p><strong>&quot;Attention is All You Need&quot;</strong> 提出的Transformer模型通过引入自注意力机制，彻底改变了我们处理序列数据的方式。它不仅在自然语言处理任务中取得了突破性的进展，还启发了许多其他领域的研究和应用。Transformer的成功在于其简洁而强大的设计，使得模型能够更有效地捕捉序列中的依赖关系，同时也开辟了模型并行化的新途径。</p><p><strong>BERT（Bidirectional Encoder Representations from Transformers）</strong> 是Google在2018年提出的一个预训练语言模型，基于Transformer架构的编码器部分。BERT的提出在自然语言处理（NLP）领域引发了巨大变革，以下是关于BERT的一些关键点：</p><h3 id="主要特点" tabindex="-1"><a class="header-anchor" href="#主要特点"><span>主要特点</span></a></h3><ol><li><p><strong>双向上下文理解</strong>：</p><ul><li>BERT使用双向Transformer，意味着它能够同时考虑词语的左边和右边的上下文信息。这与之前的模型（如Word2Vec或ELMo）不同，它们只能考虑单向上下文。</li></ul></li><li><p><strong>预训练-微调范式</strong>：</p><ul><li><strong>预训练</strong>：在大量未标注的文本数据上进行预训练，学习语言的通用表示。</li><li><strong>微调</strong>：在具体任务上使用少量标注数据进行微调，适配任务特定的需求。</li></ul></li><li><p><strong>Masked Language Model (MLM)</strong>：</p><ul><li>在预训练阶段，BERT随机地遮盖（mask）输入序列中的一些词（如15%），并训练模型预测这些被遮盖词的原始词。公式为： [ \text{MLM Loss} = -\sum_{i \in \text{Masked Tokens}} \log P(\text{original token}_i | \text{context}) ]</li></ul></li><li><p><strong>Next Sentence Prediction (NSP)</strong>：</p><ul><li>模型还被训练来预测两个句子是否是连续的，增强了模型理解句子间关系的能力。</li></ul></li><li><p><strong>Transformer架构</strong>：</p><ul><li>BERT使用了多个堆叠的Transformer编码器层，通常是12层（BERT base）或24层（BERT large）。</li></ul></li></ol><h3 id="架构" tabindex="-1"><a class="header-anchor" href="#架构"><span>架构</span></a></h3><ul><li><p><strong>输入表示</strong>：</p><ul><li>BERT将词嵌入（Word Embeddings）、段嵌入（Segment Embeddings）和位置嵌入（Position Embeddings）相加作为输入。</li></ul></li><li><p><strong>架构细节</strong>：</p><ul><li><strong>BERT base</strong>：12个编码器层，768个隐藏单元，12个注意力头，总参数量约110M。</li><li><strong>BERT large</strong>：24个编码器层，1024个隐藏单元，16个注意力头，总参数量约340M。</li></ul></li></ul><h3 id="应用" tabindex="-1"><a class="header-anchor" href="#应用"><span>应用</span></a></h3><ul><li><strong>自然语言理解</strong>：BERT在各种NLP任务中表现优异，包括但不限于问答系统、阅读理解、文本分类、命名实体识别等。</li><li><strong>生成任务</strong>：虽然BERT主要用于理解任务，但也有一些变体或基于BERT的模型用于文本生成，如BERT2BERT。</li><li><strong>迁移学习</strong>：BERT的预训练模型可以被微调到许多不同的下游任务上，减少了对大量标注数据的需求。</li></ul><h3 id="影响-1" tabindex="-1"><a class="header-anchor" href="#影响-1"><span>影响</span></a></h3><ul><li><strong>NLP范式的转变</strong>：BERT推动了预训练语言模型的发展，使得NLP任务的性能有了显著提高。</li><li><strong>模型复杂度</strong>：BERT模型的规模和复杂性带来了计算资源的挑战，但也促进了硬件和软件优化。</li><li><strong>后续工作</strong>：BERT引发了许多后续工作，如RoBERTa、ALBERT、DistilBERT等，进一步优化了模型的性能和效率。</li></ul><h3 id="总结-1" tabindex="-1"><a class="header-anchor" href="#总结-1"><span>总结</span></a></h3><p>BERT通过引入双向上下文理解和创新的预训练任务（MLM和NSP），在NLP领域取得了突破性的进展。它不仅仅是一个模型，更是一种新的学习范式，启发了许多后续研究和应用。BERT的成功在于它能够在大量未标注数据上学习到丰富的语言表示，并通过微调迅速适应各种具体的NLP任务。</p><h1 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h1><figure><img src="/assets/image-B52h57SN.png" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p><strong>Transformer</strong> 是一种用于自然语言处理（NLP）、计算机视觉（CV）等领域的深度学习模型架构，因其并行计算能力和高效的长距离依赖建模能力而广受欢迎。它最初由 Vaswani 等人在 2017 年提出，论文标题为《Attention Is All You Need》。</p><hr><h3 id="transformer-的核心思想" tabindex="-1"><a class="header-anchor" href="#transformer-的核心思想"><span>Transformer 的核心思想</span></a></h3><p>Transformer 的核心是<strong>注意力机制（Attention Mechanism）</strong>，特别是<strong>自注意力机制（Self-Attention）</strong>，用于动态计算输入序列中各位置间的重要性，从而更好地捕捉全局依赖关系。</p><hr><h3 id="transformer-的架构" tabindex="-1"><a class="header-anchor" href="#transformer-的架构"><span>Transformer 的架构</span></a></h3><p>Transformer 包含两个主要模块：</p><ol><li><strong>编码器（Encoder）</strong>： <ul><li>将输入序列编码为一组表示（高维向量）。</li></ul></li><li><strong>解码器（Decoder）</strong>： <ul><li>根据编码器的输出和目标序列，生成新的输出序列。</li></ul></li></ol><p>编码器和解码器均由多个相同的<strong>子层堆叠</strong>而成，每层包含：</p><h4 id="编码器结构" tabindex="-1"><a class="header-anchor" href="#编码器结构"><span>编码器结构</span></a></h4><ul><li><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>： <ul><li>计算序列中每个位置与其他位置的相关性。</li></ul></li><li><strong>前馈网络（Feed-Forward Network, FFN）</strong>： <ul><li>对每个位置的特征进行非线性映射。</li></ul></li><li><strong>残差连接与归一化（Residual Connection + Layer Normalization）</strong>： <ul><li>加速训练和稳定模型。</li></ul></li></ul><h4 id="解码器结构" tabindex="-1"><a class="header-anchor" href="#解码器结构"><span>解码器结构</span></a></h4><ul><li><strong>多头自注意力机制</strong>： <ul><li>只关注目标序列中已生成的部分，避免未来信息泄漏。</li></ul></li><li><strong>编码器-解码器注意力（Encoder-Decoder Attention）</strong>： <ul><li>将目标序列的每个位置与编码器输出关联。</li></ul></li><li><strong>前馈网络与残差连接</strong>（同编码器）。</li></ul><hr><h3 id="transformer-的工作流程" tabindex="-1"><a class="header-anchor" href="#transformer-的工作流程"><span>Transformer 的工作流程</span></a></h3><p>以机器翻译任务为例，Transformer 的工作流程如下：</p><ol><li><p><strong>输入处理</strong>：</p><ul><li>将词嵌入（Word Embedding）与**位置编码（Positional Encoding）**相加，加入位置信息。</li></ul></li><li><p><strong>编码器处理</strong>：</p><ul><li>输入经过堆叠的编码器层，生成一组编码后的表示。</li></ul></li><li><p><strong>解码器处理</strong>：</p><ul><li>目标序列经过解码器，结合编码器的输出生成预测。</li></ul></li><li><p><strong>输出预测</strong>：</p><ul><li>使用 softmax 生成每个时间步的词概率分布。</li></ul></li></ol><hr><h3 id="注意力机制的数学原理" tabindex="-1"><a class="header-anchor" href="#注意力机制的数学原理"><span>注意力机制的数学原理</span></a></h3><p>Transformer 的关键是<strong>缩放点积注意力（Scaled Dot-Product Attention）</strong>：</p><h4 id="输入" tabindex="-1"><a class="header-anchor" href="#输入"><span>输入：</span></a></h4><ul><li>查询向量（Query）：( Q )</li><li>键向量（Key）：( K )</li><li>值向量（Value）：( V )</li></ul><h4 id="计算" tabindex="-1"><a class="header-anchor" href="#计算"><span>计算：</span></a></h4><ol><li><p><strong>注意力得分</strong>：</p><ul><li>通过 ( Q ) 和 ( K ) 的点积计算相关性，缩放防止梯度爆炸：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex"> \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p> 其中 ( d_k ) 是键向量的维度。</li></ul></li><li><p><strong>多头注意力</strong>：</p><ul><li>使用多组 ( Q, K, V )，从不同子空间提取特征：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><msub><mtext>head</mtext><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mtext>head</mtext><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex"> \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></p> 每个注意力头：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>head</mtext><mi>i</mi></msub><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p></li></ul></li></ol><hr><h3 id="transformer-的特点和优势" tabindex="-1"><a class="header-anchor" href="#transformer-的特点和优势"><span>Transformer 的特点和优势</span></a></h3><ol><li><p><strong>并行性强</strong>：</p><ul><li>相较于 RNN，Transformer 能并行处理序列中的所有位置，显著提高训练效率。</li></ul></li><li><p><strong>长距离依赖建模能力强</strong>：</p><ul><li>通过自注意力机制，可以捕捉序列中远距离的依赖关系。</li></ul></li><li><p><strong>灵活性</strong>：</p><ul><li>可扩展到多种任务（如翻译、摘要、分类等），在 NLP 和 CV 中表现出色。</li></ul></li></ol><hr><h3 id="transformer-的应用" tabindex="-1"><a class="header-anchor" href="#transformer-的应用"><span>Transformer 的应用</span></a></h3><ol><li><p><strong>NLP 任务</strong>：</p><ul><li>机器翻译（如 Google Translate）</li><li>文本生成（如 ChatGPT）</li><li>情感分析、问答系统等</li></ul></li><li><p><strong>计算机视觉（CV）任务</strong>：</p><ul><li><strong>Vision Transformer (ViT)</strong>：将图像分割为小块，视为序列输入 Transformer。</li></ul></li><li><p><strong>跨模态任务</strong>：</p><ul><li>如图文生成、语音-文本转换。</li></ul></li></ol><hr><h3 id="transformer-的局限性" tabindex="-1"><a class="header-anchor" href="#transformer-的局限性"><span>Transformer 的局限性</span></a></h3><ol><li><p><strong>计算成本高</strong>：</p><ul><li>自注意力的计算复杂度是 ( O(n^2) )，对长序列训练不友好。</li></ul></li><li><p><strong>内存需求大</strong>：</p><ul><li>随着序列长度增加，内存消耗显著增大。</li></ul></li><li><p><strong>不适合小数据集</strong>：</p><ul><li>Transformer 需要大量数据进行训练，少数据场景下性能可能不如传统模型。</li></ul></li></ol><hr><h3 id="transformer-的发展" tabindex="-1"><a class="header-anchor" href="#transformer-的发展"><span>Transformer 的发展</span></a></h3><ol><li><p><strong>BERT（2018）</strong>：</p><ul><li>预训练双向 Transformer，用于句子表示。</li></ul></li><li><p><strong>GPT（2018 起）</strong>：</p><ul><li>生成式 Transformer，擅长文本生成。</li></ul></li><li><p><strong>ViT（2020）</strong>：</p><ul><li>将 Transformer 引入计算机视觉。</li></ul></li><li><p><strong>Efficient Transformers</strong>：</p><ul><li>如 Longformer、Reformer 等，优化了 Transformer 在长序列上的效率。</li></ul></li></ol><hr><h3 id="总结-2" tabindex="-1"><a class="header-anchor" href="#总结-2"><span>总结</span></a></h3><p>Transformer 是现代深度学习中极具影响力的模型架构，通过自注意力机制高效建模序列中的全局依赖关系。尽管其计算复杂度较高，但 Transformer 在 NLP 和 CV 中的表现已证明了其强大的能力。</p><h1 id="注意力机制-attention" tabindex="-1"><a class="header-anchor" href="#注意力机制-attention"><span>注意力机制 Attention</span></a></h1><h3 id="attention-注意力机制" tabindex="-1"><a class="header-anchor" href="#attention-注意力机制"><span><strong>Attention（注意力机制）</strong></span></a></h3><p>Attention 是深度学习中一种能够动态关注输入中最重要部分的机制，最初被提出用于机器翻译任务（Bahdanau et al., 2014），后来在 Transformer 中成为核心模块，广泛应用于 NLP、CV 和多模态任务中。</p><hr><h3 id="attention-的直观理解" tabindex="-1"><a class="header-anchor" href="#attention-的直观理解"><span><strong>Attention 的直观理解</strong></span></a></h3><p>当人类处理长文本或观察复杂场景时，往往会集中注意力于某些关键部分，而忽略无关信息。Attention 机制模仿了这种选择性注意力的过程，通过分配权重来突出输入中重要的信息。</p><hr><h3 id="attention-的数学公式" tabindex="-1"><a class="header-anchor" href="#attention-的数学公式"><span><strong>Attention 的数学公式</strong></span></a></h3><p>给定以下输入：</p><ul><li><strong>查询向量（Query）</strong>：( Q )，表示要寻找的“目标”。</li><li><strong>键向量（Key）</strong>：( K )，表示参考的“线索”。</li><li><strong>值向量（Value）</strong>：( V )，表示与键对应的信息。</li></ul><p>Attention 的输出是值向量 ( V ) 的加权和，权重由查询和键的相关性决定。具体步骤如下：</p><ol><li><p><strong>计算相关性得分</strong>：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>score</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \text{score}(Q, K) = Q K^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">score</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0858em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p></li><li><p><strong>缩放与归一化</strong>：</p><ul><li>为了避免向量维度过大导致得分值过大，使用缩放因子 ( \sqrt{d_k} )：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>scaled_score</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex"> \text{scaled\_score}(Q, K) = \frac{Q K^T}{\sqrt{d_k}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">scaled_score</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p></li><li>使用 softmax 函数将得分归一化为权重：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mtext>scaled_score</mtext><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><munder><mo>∑</mo><mi>k</mi></munder><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mtext>scaled_score</mtext><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \alpha_{ij} = \frac{\exp(\text{scaled\_score}_{ij})}{\sum_{k} \exp(\text{scaled\_score}_{ik})} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.6815em;vertical-align:-1.0457em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6358em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">scaled_score</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1264em;"><span style="top:-2.3403em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ik</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3597em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.8858em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">scaled_score</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.102em;"><span style="top:-2.3403em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4958em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0457em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p></li></ul></li><li><p><strong>加权求和</strong>：</p><ul><li>权重 ( \alpha_{ij} ) 作用于值向量 ( V )：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>V</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> \text{Attention}(Q, K, V) = \sum_j \alpha_{ij} V_j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4638em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span></p></li></ul></li></ol><hr><h3 id="attention-的分类" tabindex="-1"><a class="header-anchor" href="#attention-的分类"><span><strong>Attention 的分类</strong></span></a></h3><h4 id="_1-soft-attention-软注意力" tabindex="-1"><a class="header-anchor" href="#_1-soft-attention-软注意力"><span>1. <strong>Soft Attention（软注意力）</strong>：</span></a></h4><ul><li>输出是所有值的加权和，权重连续可微。</li><li>常用于 NLP 和 CV 中的序列建模。</li></ul><h4 id="_2-hard-attention-硬注意力" tabindex="-1"><a class="header-anchor" href="#_2-hard-attention-硬注意力"><span>2. <strong>Hard Attention（硬注意力）</strong>：</span></a></h4><ul><li>选择单个或少量关键值，非连续，需通过采样或强化学习优化。</li><li>应用于稀疏场景，但优化难度较高。</li></ul><h4 id="_3-自注意力-self-attention" tabindex="-1"><a class="header-anchor" href="#_3-自注意力-self-attention"><span>3. <strong>自注意力（Self-Attention）</strong>：</span></a></h4><ul><li>查询、键和值都来自同一输入序列，捕捉序列内的全局依赖。</li><li>是 Transformer 的核心机制。</li></ul><h4 id="_4-多头注意力-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_4-多头注意力-multi-head-attention"><span>4. <strong>多头注意力（Multi-Head Attention）</strong>：</span></a></h4><ul><li>在多个子空间并行计算注意力，以捕捉更多细粒度的相关性。</li></ul><hr><h3 id="自注意力机制-self-attention" tabindex="-1"><a class="header-anchor" href="#自注意力机制-self-attention"><span><strong>自注意力机制（Self-Attention）</strong></span></a></h3><p>在自注意力中：</p><ul><li>( Q, K, V ) 都来源于同一输入序列。</li><li>每个位置的表示会动态与所有其他位置进行关联，计算其重要性。</li></ul><h4 id="自注意力的计算流程" tabindex="-1"><a class="header-anchor" href="#自注意力的计算流程"><span>自注意力的计算流程：</span></a></h4><ol><li><p>将输入向量 ( X ) 转化为 ( Q, K, V )：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>Q</mi></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>K</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>K</mi></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>V</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>V</mi></msup></mrow><annotation encoding="application/x-tex"> Q = XW^Q, \quad K = XW^K, \quad V = XW^V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0858em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0858em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span></span></span></span></span></span></span></span></p><p>其中 ( W^Q, W^K, W^V ) 是可学习的参数矩阵。</p></li><li><p>按上述公式计算注意力输出：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex"> \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p></li><li><p>多头注意力（Multi-Head Attention）则将多个头的结果拼接并线性映射：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><msub><mtext>head</mtext><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mtext>head</mtext><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex"> \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></p><p>每个头的计算：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>head</mtext><mi>i</mi></msub><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p></li></ol><hr><h3 id="注意力的优势" tabindex="-1"><a class="header-anchor" href="#注意力的优势"><span><strong>注意力的优势</strong></span></a></h3><ol><li><p><strong>捕捉长距离依赖</strong>：</p><ul><li>不同于 RNN 的递归计算，Attention 机制能直接建模输入中任意位置的依赖。</li></ul></li><li><p><strong>并行化计算</strong>：</p><ul><li>通过矩阵运算，Attention 机制能高效并行处理序列中的所有位置。</li></ul></li><li><p><strong>可解释性强</strong>：</p><ul><li>注意力权重可以直观反映模型对输入各部分的关注程度。</li></ul></li></ol><hr><h3 id="注意力的局限性" tabindex="-1"><a class="header-anchor" href="#注意力的局限性"><span><strong>注意力的局限性</strong></span></a></h3><ol><li><p><strong>计算复杂度高</strong>：</p><ul><li>自注意力的计算复杂度为 ( O(n^2) )，当序列长度较长时会导致显著的资源消耗。</li></ul></li><li><p><strong>缺乏序列信息</strong>：</p><ul><li>Attention 本身不显式包含位置信息，需额外引入位置编码（Positional Encoding）。</li></ul></li></ol><hr><h3 id="注意力的应用场景" tabindex="-1"><a class="header-anchor" href="#注意力的应用场景"><span><strong>注意力的应用场景</strong></span></a></h3><ol><li><p><strong>自然语言处理</strong>：</p><ul><li>机器翻译、文本摘要、问答系统等。</li></ul></li><li><p><strong>计算机视觉</strong>：</p><ul><li>目标检测、图像分割（如 Vision Transformer）。</li></ul></li><li><p><strong>跨模态任务</strong>：</p><ul><li>图文生成、语音-文本转换等。</li></ul></li></ol><hr><h3 id="总结-3" tabindex="-1"><a class="header-anchor" href="#总结-3"><span><strong>总结</strong></span></a></h3><p>Attention 是深度学习中的革命性技术，特别是在序列建模任务中。通过动态关注输入中的关键部分，Attention 不仅能捕捉全局依赖，还能提高计算效率。其在 Transformer 中的成功应用，使其成为现代深度学习模型的基础模块之一。</p></div><!----><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/posts/AI/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.html" aria-label="神经网络结构"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->神经网络结构</div></a><!----></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">默认页脚</div><div class="vp-copyright">Copyright © 2024 HideOne </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-DROX2scZ.js" defer></script>
  </body>
</html>
