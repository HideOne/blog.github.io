import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as t,o as l}from"./app-CmiSf2TY.js";const e="/assets/image-B52h57SN.png",p="/assets/image-1-DSsnZsMx.png",i={};function m(r,s){return l(),n("div",null,s[0]||(s[0]=[t('<p><strong>&quot;Attention is All You Need&quot;</strong> 是2017年由Google Brain团队发表的一篇里程碑式论文，提出了Transformer模型架构。这个模型引入了注意力机制（Attention Mechanism），彻底改变了自然语言处理（NLP）领域的格局。以下是该论文的主要要点和影响：</p><h3 id="主要贡献" tabindex="-1"><a class="header-anchor" href="#主要贡献"><span>主要贡献</span></a></h3><ol><li><p><strong>纯注意力架构</strong>：</p><ul><li>Transformer完全摒弃了传统的RNN/LSTM结构，仅使用自注意力机制（Self-Attention）来处理输入序列的依赖关系。</li><li>这种设计使得模型能够并行化处理输入，极大地加速了训练过程。</li></ul></li><li><p><strong>自注意力机制（Self-Attention）</strong>：</p><ul><li>通过计算输入序列中的每个词与其他所有词之间的关系，捕捉全局依赖。</li><li>公式为： [ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V ] 其中，( Q ), ( K ), ( V ) 分别是查询（Query）、键（Key）和值（Value），( d_k ) 是键的维度。</li></ul></li><li><p><strong>多头注意力（Multi-Head Attention）</strong>：</p><ul><li>通过并行计算多个注意力机制，模型可以从不同的表示空间中获取信息，增强了模型的表达能力。</li></ul></li><li><p><strong>位置编码（Positional Encoding）</strong>：</p><ul><li>由于Transformer没有时间序列的内置顺序信息，通过添加位置编码来保留序列中单词的位置信息。</li></ul></li><li><p><strong>Encoder-Decoder结构</strong>：</p><ul><li>Transformer由编码器和解码器组成，编码器处理输入序列，解码器生成输出序列。</li></ul></li></ol><h3 id="影响" tabindex="-1"><a class="header-anchor" href="#影响"><span>影响</span></a></h3><ul><li><strong>并行计算</strong>：Transformer的设计允许在训练和推理时进行并行计算，大大减少了训练时间。</li><li><strong>长距离依赖</strong>：自注意力机制能够捕捉长距离依赖，这在传统的RNN中是一个挑战。</li><li><strong>NLP领域的变革</strong>：Transformer成为了许多后续NLP模型的基础，如BERT、RoBERTa、XLNet等。</li><li><strong>广泛应用</strong>：不仅仅是NLP，Transformer也被应用于计算机视觉、语音处理等领域，证明了其通用性和强大的表现力。</li></ul><h3 id="关键点" tabindex="-1"><a class="header-anchor" href="#关键点"><span>关键点</span></a></h3><ul><li><strong>无递归结构</strong>：Transformer不使用递归或卷积来处理序列数据，而是通过自注意力机制捕获序列中的依赖关系。</li><li><strong>可扩展性</strong>：由于其并行处理能力，Transformer可以轻松处理更长序列的数据。</li><li><strong>注意力可视化</strong>：自注意力机制提供了直观的理解方式，研究人员可以可视化不同词语之间的注意力权重。</li></ul><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><p><strong>&quot;Attention is All You Need&quot;</strong> 提出的Transformer模型通过引入自注意力机制，彻底改变了我们处理序列数据的方式。它不仅在自然语言处理任务中取得了突破性的进展，还启发了许多其他领域的研究和应用。Transformer的成功在于其简洁而强大的设计，使得模型能够更有效地捕捉序列中的依赖关系，同时也开辟了模型并行化的新途径。</p><p><strong>BERT（Bidirectional Encoder Representations from Transformers）</strong> 是Google在2018年提出的一个预训练语言模型，基于Transformer架构的编码器部分。BERT的提出在自然语言处理（NLP）领域引发了巨大变革，以下是关于BERT的一些关键点：</p><h3 id="主要特点" tabindex="-1"><a class="header-anchor" href="#主要特点"><span>主要特点</span></a></h3><ol><li><p><strong>双向上下文理解</strong>：</p><ul><li>BERT使用双向Transformer，意味着它能够同时考虑词语的左边和右边的上下文信息。这与之前的模型（如Word2Vec或ELMo）不同，它们只能考虑单向上下文。</li></ul></li><li><p><strong>预训练-微调范式</strong>：</p><ul><li><strong>预训练</strong>：在大量未标注的文本数据上进行预训练，学习语言的通用表示。</li><li><strong>微调</strong>：在具体任务上使用少量标注数据进行微调，适配任务特定的需求。</li></ul></li><li><p><strong>Masked Language Model (MLM)</strong>：</p><ul><li>在预训练阶段，BERT随机地遮盖（mask）输入序列中的一些词（如15%），并训练模型预测这些被遮盖词的原始词。公式为： [ \\text{MLM Loss} = -\\sum_{i \\in \\text{Masked Tokens}} \\log P(\\text{original token}_i | \\text{context}) ]</li></ul></li><li><p><strong>Next Sentence Prediction (NSP)</strong>：</p><ul><li>模型还被训练来预测两个句子是否是连续的，增强了模型理解句子间关系的能力。</li></ul></li><li><p><strong>Transformer架构</strong>：</p><ul><li>BERT使用了多个堆叠的Transformer编码器层，通常是12层（BERT base）或24层（BERT large）。</li></ul></li></ol><h3 id="架构" tabindex="-1"><a class="header-anchor" href="#架构"><span>架构</span></a></h3><ul><li><p><strong>输入表示</strong>：</p><ul><li>BERT将词嵌入（Word Embeddings）、段嵌入（Segment Embeddings）和位置嵌入（Position Embeddings）相加作为输入。</li></ul></li><li><p><strong>架构细节</strong>：</p><ul><li><strong>BERT base</strong>：12个编码器层，768个隐藏单元，12个注意力头，总参数量约110M。</li><li><strong>BERT large</strong>：24个编码器层，1024个隐藏单元，16个注意力头，总参数量约340M。</li></ul></li></ul><h3 id="应用" tabindex="-1"><a class="header-anchor" href="#应用"><span>应用</span></a></h3><ul><li><strong>自然语言理解</strong>：BERT在各种NLP任务中表现优异，包括但不限于问答系统、阅读理解、文本分类、命名实体识别等。</li><li><strong>生成任务</strong>：虽然BERT主要用于理解任务，但也有一些变体或基于BERT的模型用于文本生成，如BERT2BERT。</li><li><strong>迁移学习</strong>：BERT的预训练模型可以被微调到许多不同的下游任务上，减少了对大量标注数据的需求。</li></ul><h3 id="影响-1" tabindex="-1"><a class="header-anchor" href="#影响-1"><span>影响</span></a></h3><ul><li><strong>NLP范式的转变</strong>：BERT推动了预训练语言模型的发展，使得NLP任务的性能有了显著提高。</li><li><strong>模型复杂度</strong>：BERT模型的规模和复杂性带来了计算资源的挑战，但也促进了硬件和软件优化。</li><li><strong>后续工作</strong>：BERT引发了许多后续工作，如RoBERTa、ALBERT、DistilBERT等，进一步优化了模型的性能和效率。</li></ul><h3 id="总结-1" tabindex="-1"><a class="header-anchor" href="#总结-1"><span>总结</span></a></h3><p>BERT通过引入双向上下文理解和创新的预训练任务（MLM和NSP），在NLP领域取得了突破性的进展。它不仅仅是一个模型，更是一种新的学习范式，启发了许多后续研究和应用。BERT的成功在于它能够在大量未标注数据上学习到丰富的语言表示，并通过微调迅速适应各种具体的NLP任务。</p><h1 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h1><figure><img src="'+e+`" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p><strong>Transformer</strong> 是一种用于自然语言处理（NLP）、计算机视觉（CV）等领域的深度学习模型架构，因其并行计算能力和高效的长距离依赖建模能力而广受欢迎。它最初由 Vaswani 等人在 2017 年提出，论文标题为《Attention Is All You Need》。</p><hr><h3 id="transformer-的核心思想" tabindex="-1"><a class="header-anchor" href="#transformer-的核心思想"><span>Transformer 的核心思想</span></a></h3><p>Transformer 的核心是<strong>注意力机制（Attention Mechanism）</strong>，特别是<strong>自注意力机制（Self-Attention）</strong>，用于动态计算输入序列中各位置间的重要性，从而更好地捕捉全局依赖关系。</p><hr><h3 id="transformer-的架构" tabindex="-1"><a class="header-anchor" href="#transformer-的架构"><span>Transformer 的架构</span></a></h3><p>Transformer 包含两个主要模块：</p><ol><li><strong>编码器（Encoder）</strong>： <ul><li>将输入序列编码为一组表示（高维向量）。</li></ul></li><li><strong>解码器（Decoder）</strong>： <ul><li>根据编码器的输出和目标序列，生成新的输出序列。</li></ul></li></ol><p>编码器和解码器均由多个相同的<strong>子层堆叠</strong>而成，每层包含：</p><h4 id="编码器结构" tabindex="-1"><a class="header-anchor" href="#编码器结构"><span>编码器结构</span></a></h4><ul><li><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>： <ul><li>计算序列中每个位置与其他位置的相关性。</li></ul></li><li><strong>前馈网络（Feed-Forward Network, FFN）</strong>： <ul><li>对每个位置的特征进行非线性映射。</li></ul></li><li><strong>残差连接与归一化（Residual Connection + Layer Normalization）</strong>： <ul><li>加速训练和稳定模型。</li></ul></li></ul><h4 id="解码器结构" tabindex="-1"><a class="header-anchor" href="#解码器结构"><span>解码器结构</span></a></h4><ul><li><strong>多头自注意力机制</strong>： <ul><li>只关注目标序列中已生成的部分，避免未来信息泄漏。</li></ul></li><li><strong>编码器-解码器注意力（Encoder-Decoder Attention）</strong>： <ul><li>将目标序列的每个位置与编码器输出关联。</li></ul></li><li><strong>前馈网络与残差连接</strong>（同编码器）。</li></ul><hr><h3 id="transformer-的工作流程" tabindex="-1"><a class="header-anchor" href="#transformer-的工作流程"><span>Transformer 的工作流程</span></a></h3><p>以机器翻译任务为例，Transformer 的工作流程如下：</p><ol><li><p><strong>输入处理</strong>：</p><ul><li>将词嵌入（Word Embedding）与**位置编码（Positional Encoding）**相加，加入位置信息。</li></ul></li><li><p><strong>编码器处理</strong>：</p><ul><li>输入经过堆叠的编码器层，生成一组编码后的表示。</li></ul></li><li><p><strong>解码器处理</strong>：</p><ul><li>目标序列经过解码器，结合编码器的输出生成预测。</li></ul></li><li><p><strong>输出预测</strong>：</p><ul><li>使用 softmax 生成每个时间步的词概率分布。</li></ul></li></ol><hr><h3 id="注意力机制的数学原理" tabindex="-1"><a class="header-anchor" href="#注意力机制的数学原理"><span>注意力机制的数学原理</span></a></h3><p>Transformer 的关键是<strong>缩放点积注意力（Scaled Dot-Product Attention）</strong>：</p><h4 id="输入" tabindex="-1"><a class="header-anchor" href="#输入"><span>输入：</span></a></h4><ul><li>查询向量（Query）：( Q )</li><li>键向量（Key）：( K )</li><li>值向量（Value）：( V )</li></ul><h4 id="计算" tabindex="-1"><a class="header-anchor" href="#计算"><span>计算：</span></a></h4><ol><li><p><strong>注意力得分</strong>：</p><ul><li>通过 ( Q ) 和 ( K ) 的点积计算相关性，缩放防止梯度爆炸：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex"> \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p> 其中 ( d_k ) 是键向量的维度。</li></ul></li><li><p><strong>多头注意力</strong>：</p><ul><li>使用多组 ( Q, K, V )，从不同子空间提取特征：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><msub><mtext>head</mtext><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mtext>head</mtext><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex"> \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></p> 每个注意力头：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>head</mtext><mi>i</mi></msub><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p></li></ul></li></ol><hr><h3 id="transformer-的特点和优势" tabindex="-1"><a class="header-anchor" href="#transformer-的特点和优势"><span>Transformer 的特点和优势</span></a></h3><ol><li><p><strong>并行性强</strong>：</p><ul><li>相较于 RNN，Transformer 能并行处理序列中的所有位置，显著提高训练效率。</li></ul></li><li><p><strong>长距离依赖建模能力强</strong>：</p><ul><li>通过自注意力机制，可以捕捉序列中远距离的依赖关系。</li></ul></li><li><p><strong>灵活性</strong>：</p><ul><li>可扩展到多种任务（如翻译、摘要、分类等），在 NLP 和 CV 中表现出色。</li></ul></li></ol><hr><h3 id="transformer-的应用" tabindex="-1"><a class="header-anchor" href="#transformer-的应用"><span>Transformer 的应用</span></a></h3><ol><li><p><strong>NLP 任务</strong>：</p><ul><li>机器翻译（如 Google Translate）</li><li>文本生成（如 ChatGPT）</li><li>情感分析、问答系统等</li></ul></li><li><p><strong>计算机视觉（CV）任务</strong>：</p><ul><li><strong>Vision Transformer (ViT)</strong>：将图像分割为小块，视为序列输入 Transformer。</li></ul></li><li><p><strong>跨模态任务</strong>：</p><ul><li>如图文生成、语音-文本转换。</li></ul></li></ol><hr><h3 id="transformer-的局限性" tabindex="-1"><a class="header-anchor" href="#transformer-的局限性"><span>Transformer 的局限性</span></a></h3><ol><li><p><strong>计算成本高</strong>：</p><ul><li>自注意力的计算复杂度是 ( O(n^2) )，对长序列训练不友好。</li></ul></li><li><p><strong>内存需求大</strong>：</p><ul><li>随着序列长度增加，内存消耗显著增大。</li></ul></li><li><p><strong>不适合小数据集</strong>：</p><ul><li>Transformer 需要大量数据进行训练，少数据场景下性能可能不如传统模型。</li></ul></li></ol><hr><h3 id="transformer-的发展" tabindex="-1"><a class="header-anchor" href="#transformer-的发展"><span>Transformer 的发展</span></a></h3><ol><li><p><strong>BERT（2018）</strong>：</p><ul><li>预训练双向 Transformer，用于句子表示。</li></ul></li><li><p><strong>GPT（2018 起）</strong>：</p><ul><li>生成式 Transformer，擅长文本生成。</li></ul></li><li><p><strong>ViT（2020）</strong>：</p><ul><li>将 Transformer 引入计算机视觉。</li></ul></li><li><p><strong>Efficient Transformers</strong>：</p><ul><li>如 Longformer、Reformer 等，优化了 Transformer 在长序列上的效率。</li></ul></li></ol><hr><h3 id="总结-2" tabindex="-1"><a class="header-anchor" href="#总结-2"><span>总结</span></a></h3><p>Transformer 是现代深度学习中极具影响力的模型架构，通过自注意力机制高效建模序列中的全局依赖关系。尽管其计算复杂度较高，但 Transformer 在 NLP 和 CV 中的表现已证明了其强大的能力。</p><h1 id="注意力机制-attention" tabindex="-1"><a class="header-anchor" href="#注意力机制-attention"><span>注意力机制 Attention</span></a></h1><h3 id="attention-注意力机制" tabindex="-1"><a class="header-anchor" href="#attention-注意力机制"><span><strong>Attention（注意力机制）</strong></span></a></h3><p>Attention 是深度学习中一种能够动态关注输入中最重要部分的机制，最初被提出用于机器翻译任务（Bahdanau et al., 2014），后来在 Transformer 中成为核心模块，广泛应用于 NLP、CV 和多模态任务中。</p><hr><h3 id="attention-的直观理解" tabindex="-1"><a class="header-anchor" href="#attention-的直观理解"><span><strong>Attention 的直观理解</strong></span></a></h3><p>当人类处理长文本或观察复杂场景时，往往会集中注意力于某些关键部分，而忽略无关信息。Attention 机制模仿了这种选择性注意力的过程，通过分配权重来突出输入中重要的信息。</p><hr><h3 id="attention-的数学公式" tabindex="-1"><a class="header-anchor" href="#attention-的数学公式"><span><strong>Attention 的数学公式</strong></span></a></h3><p>给定以下输入：</p><ul><li><strong>查询向量（Query）</strong>：( Q )，表示要寻找的“目标”。</li><li><strong>键向量（Key）</strong>：( K )，表示参考的“线索”。</li><li><strong>值向量（Value）</strong>：( V )，表示与键对应的信息。</li></ul><p>Attention 的输出是值向量 ( V ) 的加权和，权重由查询和键的相关性决定。具体步骤如下：</p><ol><li><p><strong>计算相关性得分</strong>：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>score</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\text{score}(Q, K) = Q K^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">score</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0858em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p></li><li><p><strong>缩放与归一化</strong>：</p><ul><li>为了避免向量维度过大导致得分值过大，使用缩放因子 ( \\sqrt{d_k} )：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>scaled_score</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex"> \\text{scaled\\_score}(Q, K) = \\frac{Q K^T}{\\sqrt{d_k}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">scaled_score</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p></li><li>使用 softmax 函数将得分归一化为权重：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mtext>scaled_score</mtext><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><munder><mo>∑</mo><mi>k</mi></munder><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mtext>scaled_score</mtext><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\alpha_{ij} = \\frac{\\exp(\\text{scaled\\_score}_{ij})}{\\sum_{k} \\exp(\\text{scaled\\_score}_{ik})} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.6815em;vertical-align:-1.0457em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6358em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">scaled_score</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1264em;"><span style="top:-2.3403em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ik</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3597em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.8858em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">scaled_score</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.102em;"><span style="top:-2.3403em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4958em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0457em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p></li></ul></li><li><p><strong>加权求和</strong>：</p><ul><li>权重 ( \\alpha_{ij} ) 作用于值向量 ( V )：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>V</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> \\text{Attention}(Q, K, V) = \\sum_j \\alpha_{ij} V_j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4638em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span></p></li></ul></li></ol><hr><h3 id="attention-的分类" tabindex="-1"><a class="header-anchor" href="#attention-的分类"><span><strong>Attention 的分类</strong></span></a></h3><h4 id="_1-soft-attention-软注意力" tabindex="-1"><a class="header-anchor" href="#_1-soft-attention-软注意力"><span>1. <strong>Soft Attention（软注意力）</strong>：</span></a></h4><ul><li>输出是所有值的加权和，权重连续可微。</li><li>常用于 NLP 和 CV 中的序列建模。</li></ul><h4 id="_2-hard-attention-硬注意力" tabindex="-1"><a class="header-anchor" href="#_2-hard-attention-硬注意力"><span>2. <strong>Hard Attention（硬注意力）</strong>：</span></a></h4><ul><li>选择单个或少量关键值，非连续，需通过采样或强化学习优化。</li><li>应用于稀疏场景，但优化难度较高。</li></ul><h4 id="_3-自注意力-self-attention" tabindex="-1"><a class="header-anchor" href="#_3-自注意力-self-attention"><span>3. <strong>自注意力（Self-Attention）</strong>：</span></a></h4><ul><li>查询、键和值都来自同一输入序列，捕捉序列内的全局依赖。</li><li>是 Transformer 的核心机制。</li></ul><h4 id="_4-多头注意力-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_4-多头注意力-multi-head-attention"><span>4. <strong>多头注意力（Multi-Head Attention）</strong>：</span></a></h4><ul><li>在多个子空间并行计算注意力，以捕捉更多细粒度的相关性。</li></ul><hr><h3 id="自注意力机制-self-attention" tabindex="-1"><a class="header-anchor" href="#自注意力机制-self-attention"><span><strong>自注意力机制（Self-Attention）</strong></span></a></h3><p>在自注意力中：</p><ul><li>( Q, K, V ) 都来源于同一输入序列。</li><li>每个位置的表示会动态与所有其他位置进行关联，计算其重要性。</li></ul><h4 id="自注意力的计算流程" tabindex="-1"><a class="header-anchor" href="#自注意力的计算流程"><span>自注意力的计算流程：</span></a></h4><ol><li><p>将输入向量 ( X ) 转化为 ( Q, K, V )：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>Q</mi></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>K</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>K</mi></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>V</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>V</mi></msup></mrow><annotation encoding="application/x-tex"> Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0858em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0858em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span></span></span></span></span></span></span></span></p><p>其中 ( W^Q, W^K, W^V ) 是可学习的参数矩阵。</p></li><li><p>按上述公式计算注意力输出：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex"> \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p></li><li><p>多头注意力（Multi-Head Attention）则将多个头的结果拼接并线性映射：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><msub><mtext>head</mtext><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mtext>head</mtext><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex"> \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></p><p>每个头的计算：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>head</mtext><mi>i</mi></msub><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p></li></ol><hr><h3 id="注意力的优势" tabindex="-1"><a class="header-anchor" href="#注意力的优势"><span><strong>注意力的优势</strong></span></a></h3><ol><li><p><strong>捕捉长距离依赖</strong>：</p><ul><li>不同于 RNN 的递归计算，Attention 机制能直接建模输入中任意位置的依赖。</li></ul></li><li><p><strong>并行化计算</strong>：</p><ul><li>通过矩阵运算，Attention 机制能高效并行处理序列中的所有位置。</li></ul></li><li><p><strong>可解释性强</strong>：</p><ul><li>注意力权重可以直观反映模型对输入各部分的关注程度。</li></ul></li></ol><hr><h3 id="注意力的局限性" tabindex="-1"><a class="header-anchor" href="#注意力的局限性"><span><strong>注意力的局限性</strong></span></a></h3><ol><li><p><strong>计算复杂度高</strong>：</p><ul><li>自注意力的计算复杂度为 ( O(n^2) )，当序列长度较长时会导致显著的资源消耗。</li></ul></li><li><p><strong>缺乏序列信息</strong>：</p><ul><li>Attention 本身不显式包含位置信息，需额外引入位置编码（Positional Encoding）。</li></ul></li></ol><hr><h3 id="注意力的应用场景" tabindex="-1"><a class="header-anchor" href="#注意力的应用场景"><span><strong>注意力的应用场景</strong></span></a></h3><ol><li><p><strong>自然语言处理</strong>：</p><ul><li>机器翻译、文本摘要、问答系统等。</li></ul></li><li><p><strong>计算机视觉</strong>：</p><ul><li>目标检测、图像分割（如 Vision Transformer）。</li></ul></li><li><p><strong>跨模态任务</strong>：</p><ul><li>图文生成、语音-文本转换等。</li></ul></li></ol><hr><h3 id="总结-3" tabindex="-1"><a class="header-anchor" href="#总结-3"><span><strong>总结</strong></span></a></h3><p>Attention 是深度学习中的革命性技术，特别是在序列建模任务中。通过动态关注输入中的关键部分，Attention 不仅能捕捉全局依赖，还能提高计算效率。其在 Transformer 中的成功应用，使其成为现代深度学习模型的基础模块之一。</p><h2 id="self-attention如何计算" tabindex="-1"><a class="header-anchor" href="#self-attention如何计算"><span>self-attention如何计算</span></a></h2><figure><img src="`+p+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure>',103)]))}const h=a(i,[["render",m],["__file","Transform.html.vue"]]),g=JSON.parse('{"path":"/posts/AI/Transform.html","title":"Transform","lang":"zh-CN","frontmatter":{"title":"Transform","date":"2024-11-25T22:18:25.000Z","headerDepth":2,"description":"\\"Attention is All You Need\\" 是2017年由Google Brain团队发表的一篇里程碑式论文，提出了Transformer模型架构。这个模型引入了注意力机制（Attention Mechanism），彻底改变了自然语言处理（NLP）领域的格局。以下是该论文的主要要点和影响： 主要贡献 纯注意力架构： Transformer完...","head":[["meta",{"property":"og:url","content":"https://blog.vipfreevpn.top/posts/AI/Transform.html"}],["meta",{"property":"og:site_name","content":"博客"}],["meta",{"property":"og:title","content":"Transform"}],["meta",{"property":"og:description","content":"\\"Attention is All You Need\\" 是2017年由Google Brain团队发表的一篇里程碑式论文，提出了Transformer模型架构。这个模型引入了注意力机制（Attention Mechanism），彻底改变了自然语言处理（NLP）领域的格局。以下是该论文的主要要点和影响： 主要贡献 纯注意力架构： Transformer完..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-11-26T09:36:11.000Z"}],["meta",{"property":"article:published_time","content":"2024-11-25T22:18:25.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-26T09:36:11.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transform\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-11-25T22:18:25.000Z\\",\\"dateModified\\":\\"2024-11-26T09:36:11.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"HideOne\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":3,"title":"主要贡献","slug":"主要贡献","link":"#主要贡献","children":[]},{"level":3,"title":"影响","slug":"影响","link":"#影响","children":[]},{"level":3,"title":"关键点","slug":"关键点","link":"#关键点","children":[]},{"level":3,"title":"总结","slug":"总结","link":"#总结","children":[]},{"level":3,"title":"主要特点","slug":"主要特点","link":"#主要特点","children":[]},{"level":3,"title":"架构","slug":"架构","link":"#架构","children":[]},{"level":3,"title":"应用","slug":"应用","link":"#应用","children":[]},{"level":3,"title":"影响","slug":"影响-1","link":"#影响-1","children":[]},{"level":3,"title":"总结","slug":"总结-1","link":"#总结-1","children":[]},{"level":1,"title":"Transformer","slug":"transformer","link":"#transformer","children":[{"level":3,"title":"Transformer 的核心思想","slug":"transformer-的核心思想","link":"#transformer-的核心思想","children":[]},{"level":3,"title":"Transformer 的架构","slug":"transformer-的架构","link":"#transformer-的架构","children":[{"level":4,"title":"编码器结构","slug":"编码器结构","link":"#编码器结构","children":[]},{"level":4,"title":"解码器结构","slug":"解码器结构","link":"#解码器结构","children":[]}]},{"level":3,"title":"Transformer 的工作流程","slug":"transformer-的工作流程","link":"#transformer-的工作流程","children":[]},{"level":3,"title":"注意力机制的数学原理","slug":"注意力机制的数学原理","link":"#注意力机制的数学原理","children":[{"level":4,"title":"输入：","slug":"输入","link":"#输入","children":[]},{"level":4,"title":"计算：","slug":"计算","link":"#计算","children":[]}]},{"level":3,"title":"Transformer 的特点和优势","slug":"transformer-的特点和优势","link":"#transformer-的特点和优势","children":[]},{"level":3,"title":"Transformer 的应用","slug":"transformer-的应用","link":"#transformer-的应用","children":[]},{"level":3,"title":"Transformer 的局限性","slug":"transformer-的局限性","link":"#transformer-的局限性","children":[]},{"level":3,"title":"Transformer 的发展","slug":"transformer-的发展","link":"#transformer-的发展","children":[]},{"level":3,"title":"总结","slug":"总结-2","link":"#总结-2","children":[]}]},{"level":1,"title":"注意力机制 Attention","slug":"注意力机制-attention","link":"#注意力机制-attention","children":[{"level":3,"title":"Attention（注意力机制）","slug":"attention-注意力机制","link":"#attention-注意力机制","children":[]},{"level":3,"title":"Attention 的直观理解","slug":"attention-的直观理解","link":"#attention-的直观理解","children":[]},{"level":3,"title":"Attention 的数学公式","slug":"attention-的数学公式","link":"#attention-的数学公式","children":[]},{"level":3,"title":"Attention 的分类","slug":"attention-的分类","link":"#attention-的分类","children":[{"level":4,"title":"1. Soft Attention（软注意力）：","slug":"_1-soft-attention-软注意力","link":"#_1-soft-attention-软注意力","children":[]},{"level":4,"title":"2. Hard Attention（硬注意力）：","slug":"_2-hard-attention-硬注意力","link":"#_2-hard-attention-硬注意力","children":[]},{"level":4,"title":"3. 自注意力（Self-Attention）：","slug":"_3-自注意力-self-attention","link":"#_3-自注意力-self-attention","children":[]},{"level":4,"title":"4. 多头注意力（Multi-Head Attention）：","slug":"_4-多头注意力-multi-head-attention","link":"#_4-多头注意力-multi-head-attention","children":[]}]},{"level":3,"title":"自注意力机制（Self-Attention）","slug":"自注意力机制-self-attention","link":"#自注意力机制-self-attention","children":[{"level":4,"title":"自注意力的计算流程：","slug":"自注意力的计算流程","link":"#自注意力的计算流程","children":[]}]},{"level":3,"title":"注意力的优势","slug":"注意力的优势","link":"#注意力的优势","children":[]},{"level":3,"title":"注意力的局限性","slug":"注意力的局限性","link":"#注意力的局限性","children":[]},{"level":3,"title":"注意力的应用场景","slug":"注意力的应用场景","link":"#注意力的应用场景","children":[]},{"level":3,"title":"总结","slug":"总结-3","link":"#总结-3","children":[]},{"level":2,"title":"self-attention如何计算","slug":"self-attention如何计算","link":"#self-attention如何计算","children":[]}]}],"git":{"createdTime":1732613771000,"updatedTime":1732613771000,"contributors":[{"name":"HideOne","email":"844085696@qq.com","commits":1}]},"readingTime":{"minutes":12.03,"words":3609},"filePathRelative":"posts/AI/Transform.md","localizedDate":"2024年11月26日","excerpt":"<p><strong>\\"Attention is All You Need\\"</strong> 是2017年由Google Brain团队发表的一篇里程碑式论文，提出了Transformer模型架构。这个模型引入了注意力机制（Attention Mechanism），彻底改变了自然语言处理（NLP）领域的格局。以下是该论文的主要要点和影响：</p>\\n<h3>主要贡献</h3>\\n<ol>\\n<li>\\n<p><strong>纯注意力架构</strong>：</p>\\n<ul>\\n<li>Transformer完全摒弃了传统的RNN/LSTM结构，仅使用自注意力机制（Self-Attention）来处理输入序列的依赖关系。</li>\\n<li>这种设计使得模型能够并行化处理输入，极大地加速了训练过程。</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>自注意力机制（Self-Attention）</strong>：</p>\\n<ul>\\n<li>通过计算输入序列中的每个词与其他所有词之间的关系，捕捉全局依赖。</li>\\n<li>公式为：\\n[ \\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V ]\\n其中，( Q ), ( K ), ( V ) 分别是查询（Query）、键（Key）和值（Value），( d_k ) 是键的维度。</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>多头注意力（Multi-Head Attention）</strong>：</p>\\n<ul>\\n<li>通过并行计算多个注意力机制，模型可以从不同的表示空间中获取信息，增强了模型的表达能力。</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>位置编码（Positional Encoding）</strong>：</p>\\n<ul>\\n<li>由于Transformer没有时间序列的内置顺序信息，通过添加位置编码来保留序列中单词的位置信息。</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>Encoder-Decoder结构</strong>：</p>\\n<ul>\\n<li>Transformer由编码器和解码器组成，编码器处理输入序列，解码器生成输出序列。</li>\\n</ul>\\n</li>\\n</ol>","autoDesc":true}');export{h as comp,g as data};
