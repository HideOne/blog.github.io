import{_ as a}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as n,c as s,a as t}from"./app-yAc52h7c.js";const e="/assets/image-17tlCZKU.png",o={},r=t('<h1 id="线性函数" tabindex="-1"><a class="header-anchor" href="#线性函数" aria-hidden="true">#</a> 线性函数</h1><p>y = wx + b w: 权重 b：微调</p><h1 id="损失函数" tabindex="-1"><a class="header-anchor" href="#损失函数" aria-hidden="true">#</a> 损失函数</h1><p><strong>损失函数</strong>（Loss Function）是机器学习和深度学习模型中用于衡量预测结果与实际目标之间误差的函数。通过损失函数，模型可以了解其预测结果的准确性，并根据误差大小调整模型的参数（通过优化算法如梯度下降）。</p><h3 id="损失函数的作用" tabindex="-1"><a class="header-anchor" href="#损失函数的作用" aria-hidden="true">#</a> <strong>损失函数的作用</strong></h3><ol><li><strong>衡量误差</strong>：评估模型预测结果与真实值之间的偏差。</li><li><strong>指导优化</strong>：优化算法通过最小化损失函数的值来更新模型参数，提升模型的性能。</li></ol><hr><h3 id="常见的损失函数" tabindex="-1"><a class="header-anchor" href="#常见的损失函数" aria-hidden="true">#</a> <strong>常见的损失函数</strong></h3><p>损失函数的选择取决于任务的类型（回归、分类等）和问题的性质。</p><h4 id="_1-回归问题" tabindex="-1"><a class="header-anchor" href="#_1-回归问题" aria-hidden="true">#</a> 1. <strong>回归问题</strong></h4><p>用于预测连续值的任务，常见损失函数包括：</p><ul><li><p><strong>均方误差（Mean Squared Error, MSE）</strong><br> [ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 ]<br> 用于惩罚大误差，适合对误差较为敏感的场景。</p></li><li><p><strong>平均绝对误差（Mean Absolute Error, MAE）</strong><br> [ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| ]<br> 对异常值的惩罚较轻，适合数据中有噪声的场景。</p></li></ul><h4 id="_2-分类问题" tabindex="-1"><a class="header-anchor" href="#_2-分类问题" aria-hidden="true">#</a> 2. <strong>分类问题</strong></h4><p>用于预测离散类别的任务，常见损失函数包括：</p><ul><li><p><strong>交叉熵损失（Cross-Entropy Loss）</strong><br> 对于二分类： [ \\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] ]<br> 适用于分类问题，特别是概率分布的匹配。</p></li><li><p><strong>Hinge Loss</strong><br> 用于支持向量机（SVM）： [ \\text{Loss} = \\sum_{i=1}^n \\max(0, 1 - y_i \\cdot \\hat{y}_i) ]</p></li></ul><h4 id="_3-其他损失函数" tabindex="-1"><a class="header-anchor" href="#_3-其他损失函数" aria-hidden="true">#</a> 3. <strong>其他损失函数</strong></h4><ul><li><p><strong>Huber Loss</strong><br> 结合了 MSE 和 MAE 的优点，适用于对异常值鲁棒的回归问题。</p></li><li><p><strong>Kullback-Leibler 散度（KL 散度）</strong><br> 测量两个概率分布之间的差异，常用于生成模型中。</p></li></ul><hr><h3 id="损失函数与优化目标" tabindex="-1"><a class="header-anchor" href="#损失函数与优化目标" aria-hidden="true">#</a> <strong>损失函数与优化目标</strong></h3><p><strong>目标函数</strong>是模型需要优化的整体表达式，通常等于损失函数加上正则化项： [ \\text{目标函数} = \\text{损失函数} + \\text{正则化项} ]</p><p><strong>优化过程</strong>通过最小化目标函数来找到模型参数的最优值。</p><hr><h3 id="直观理解" tabindex="-1"><a class="header-anchor" href="#直观理解" aria-hidden="true">#</a> 直观理解</h3><ul><li>损失函数的值越小，说明模型的预测结果越接近真实值。</li><li>不同任务需要选择合适的损失函数，过于复杂或不适合的损失函数可能会影响模型效果。</li><li></li></ul><figure><img src="'+e+`" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h1 id="softmax" tabindex="-1"><a class="header-anchor" href="#softmax" aria-hidden="true">#</a> softmax</h1><p><strong>Softmax</strong> 是一种归一化函数，常用于分类任务的机器学习和深度学习模型中，特别是在多分类问题中。Softmax 函数的作用是将一个任意的实数向量转换为一个概率分布，使得每个值在 [0, 1] 范围内，并且所有值的总和为 1。</p><hr><h3 id="softmax-函数定义" tabindex="-1"><a class="header-anchor" href="#softmax-函数定义" aria-hidden="true">#</a> <strong>Softmax 函数定义</strong></h3><p>对于输入向量 ( z = [z_1, z_2, \\dots, z_n] )，Softmax 函数定义如下：</p><p>[ \\text{Softmax}(z_i) = \\frac{e<sup>{z_i}}{\\sum_{j=1}</sup>n e^{z_j}}, \\quad \\text{for } i = 1, 2, \\dots, n ]</p><h3 id="性质" tabindex="-1"><a class="header-anchor" href="#性质" aria-hidden="true">#</a> <strong>性质</strong></h3><ol><li><p><strong>归一化</strong>：输出的每个分量在 ([0, 1]) 之间，并且所有分量的和为 1： [ \\sum_{i=1}^n \\text{Softmax}(z_i) = 1 ] 因此，Softmax 的输出可以解释为概率分布。</p></li><li><p><strong>相对大小保留</strong>：Softmax 保留了输入值之间的相对大小。输入值越大，Softmax 输出的概率越高。</p></li></ol><hr><h3 id="softmax-的用途" tabindex="-1"><a class="header-anchor" href="#softmax-的用途" aria-hidden="true">#</a> <strong>Softmax 的用途</strong></h3><ol><li><p><strong>多分类问题</strong>：在分类任务中，Softmax 常用于神经网络的输出层，将网络的原始输出值（logits）转换为概率分布。<br> 比如，对于一个 3 类分类问题，Softmax 输出可能是： [ [0.1, 0.7, 0.2] ] 表示属于每一类的概率。</p></li><li><p><strong>概率解释</strong>：Softmax 的输出直接用于交叉熵损失函数（Cross-Entropy Loss），来衡量预测的概率分布与真实分布之间的误差。</p></li></ol><hr><h3 id="例子" tabindex="-1"><a class="header-anchor" href="#例子" aria-hidden="true">#</a> <strong>例子</strong></h3><p>假设一个模型的输出（logits）为 ( z = [2.0, 1.0, 0.1] )，我们通过 Softmax 计算概率分布：</p><ol><li><p>计算指数值： [ e^{z_1} = e^{2.0}, \\quad e^{z_2} = e^{1.0}, \\quad e^{z_3} = e^{0.1} ]</p></li><li><p>求和： [ \\text{Sum} = e^{2.0} + e^{1.0} + e^{0.1} ]</p></li><li><p>计算概率： [ \\text{Softmax}(z_1) = \\frac{e^{2.0}}{\\text{Sum}}, \\quad \\text{Softmax}(z_2) = \\frac{e^{1.0}}{\\text{Sum}}, \\quad \\text{Softmax}(z_3) = \\frac{e^{0.1}}{\\text{Sum}} ]</p></li></ol><hr><h3 id="softmax-与-sigmoid-的对比" tabindex="-1"><a class="header-anchor" href="#softmax-与-sigmoid-的对比" aria-hidden="true">#</a> <strong>Softmax 与 Sigmoid 的对比</strong></h3><ul><li><strong>Sigmoid</strong> 用于二分类问题，将单个值映射到 ([0, 1]) 的概率范围。</li><li><strong>Softmax</strong> 用于多分类问题，将多个值映射到一个概率分布。</li></ul><hr><h3 id="实现代码" tabindex="-1"><a class="header-anchor" href="#实现代码" aria-hidden="true">#</a> <strong>实现代码</strong></h3><p>在 Python 中，Softmax 可以通过以下方式实现：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">:</span>
    exp_z <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>z <span class="token operator">-</span> np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 防止指数溢出</span>
    <span class="token keyword">return</span> exp_z <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>exp_z<span class="token punctuation">)</span>

<span class="token comment"># 示例</span>
z <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
probabilities <span class="token operator">=</span> softmax<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>probabilities<span class="token punctuation">)</span>
<span class="token comment"># 输出: [0.65900114 0.24243297 0.09856589]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>注意</strong>：这里通过减去 (\\max(z)) 来避免数值溢出问题，提高计算的稳定性。</p>`,48),i=[r];function p(l,c){return n(),s("div",null,i)}const h=a(o,[["render",p],["__file","神经网络结构.html.vue"]]);export{h as default};
